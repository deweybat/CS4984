{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, io\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces the use of dictionaries for counting the frequency of some category of words in text, using sentiment (from the [AFINN sentiment lexicon](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010)) in the time series data of tweets as an example.\n",
    "\n",
    "Checkout resources for available dictionaries. Here are a few widely used ones: [VADER](https://github.com/cjhutto/vaderSentiment), AFINN sentiment lexicon; [MPQA](https://mpqa.cs.pitt.edu/lexicons/subj_lexicon/) (free for use with registration) and [LIWC](http://liwc.wpengine.com) (commercial), [EMPATH](https://github.com/Ejhfast/empath-client)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in json file of tweets and return a list of (date, tokenized text)\n",
    "def read_tweets_from_json(filename):\n",
    "    tweets=[]\n",
    "#    with open(filename, encoding=\"utf-8\") as file:\n",
    "    with io.open(filename, encoding=\"utf-8\") as file:\n",
    "        data=json.load(file)\n",
    "        for tweet in data:\n",
    "            created_at=tweet[\"created_at\"]\n",
    "            date = pd.to_datetime(created_at)\n",
    "            text=tweet[\"text\"]\n",
    "            tokens=nltk.casual_tokenize(text)\n",
    "            tweets.append((date, tokens))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in list of (date, tokens) tweets and count whether each tweet contains \n",
    "# a (lowercased) term in the argument dictionary.  Return as pandas dataframe\n",
    "# for easier slicing/plotting)\n",
    "def dictionary_document_count(data, dictionary):\n",
    "    counted=[]\n",
    "    for date, tokens in data:\n",
    "        val=0\n",
    "        for word in tokens:\n",
    "            if word.lower() in dictionary:\n",
    "                val=1\n",
    "        counted.append((date, val))\n",
    "    df=pd.DataFrame(counted, columns=['date','document frequency'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=read_tweets_from_json(\"trump_tweets.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll define our own \"immigration\" dictionary by selecting words that we hypothesize are often found in the topic of immigration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_dictionary=set([\"wall\", \"border\", \"borders\", \"immigrants\",\"immigration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=dictionary_document_count(tweets, immigration_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     date  document frequency\n",
      "0     2019-01-19 14:09:37                   1\n",
      "1     2019-01-19 13:50:09                   0\n",
      "2     2019-01-19 12:51:30                   0\n",
      "3     2019-01-19 12:11:38                   0\n",
      "4     2019-01-19 11:29:21                   0\n",
      "5     2019-01-19 03:24:49                   0\n",
      "6     2019-01-19 03:22:44                   0\n",
      "7     2019-01-19 03:22:29                   0\n",
      "8     2019-01-19 03:02:50                   0\n",
      "9     2019-01-19 02:23:46                   0\n",
      "10    2019-01-19 02:18:18                   0\n",
      "11    2019-01-19 02:18:14                   0\n",
      "12    2019-01-19 02:14:44                   0\n",
      "13    2019-01-19 02:14:11                   0\n",
      "14    2019-01-19 01:52:47                   0\n",
      "15    2019-01-19 01:50:03                   0\n",
      "16    2019-01-19 01:46:08                   1\n",
      "17    2019-01-19 01:44:59                   0\n",
      "18    2019-01-19 01:42:51                   0\n",
      "19    2019-01-19 01:16:18                   0\n",
      "20    2019-01-19 01:14:30                   0\n",
      "21    2019-01-19 01:12:08                   0\n",
      "22    2019-01-19 01:06:38                   0\n",
      "23    2019-01-18 22:51:00                   1\n",
      "24    2019-01-18 16:58:04                   0\n",
      "25    2019-01-18 16:00:07                   0\n",
      "26    2019-01-18 15:59:35                   0\n",
      "27    2019-01-18 15:58:37                   1\n",
      "28    2019-01-18 15:02:41                   1\n",
      "29    2019-01-18 14:13:56                   1\n",
      "...                   ...                 ...\n",
      "36553 2009-06-24 22:09:19                   0\n",
      "36554 2009-06-23 13:40:38                   0\n",
      "36555 2009-06-21 14:47:41                   0\n",
      "36556 2009-06-18 13:26:53                   0\n",
      "36557 2009-06-15 23:13:05                   0\n",
      "36558 2009-06-14 14:25:36                   0\n",
      "36559 2009-06-08 20:15:29                   0\n",
      "36560 2009-06-05 18:21:37                   0\n",
      "36561 2009-06-03 18:19:49                   0\n",
      "36562 2009-05-28 18:03:34                   0\n",
      "36563 2009-05-27 14:18:52                   0\n",
      "36564 2009-05-26 14:42:01                   0\n",
      "36565 2009-05-23 16:11:19                   0\n",
      "36566 2009-05-22 16:28:34                   0\n",
      "36567 2009-05-22 02:59:39                   0\n",
      "36568 2009-05-20 22:29:47                   0\n",
      "36569 2009-05-20 13:25:39                   0\n",
      "36570 2009-05-19 17:43:39                   0\n",
      "36571 2009-05-18 14:26:00                   0\n",
      "36572 2009-05-17 15:00:03                   0\n",
      "36573 2009-05-16 22:22:45                   0\n",
      "36574 2009-05-15 14:13:13                   0\n",
      "36575 2009-05-14 16:30:40                   0\n",
      "36576 2009-05-13 17:38:28                   0\n",
      "36577 2009-05-12 19:21:55                   0\n",
      "36578 2009-05-12 14:07:28                   0\n",
      "36579 2009-05-08 20:40:15                   0\n",
      "36580 2009-05-08 13:38:08                   0\n",
      "36581 2009-05-05 01:00:10                   0\n",
      "36582 2009-05-04 18:54:25                   0\n",
      "\n",
      "[36583 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AFINN dictionary is a sentiment lexicon, where words are rated on a five-point affect scale (-5 = most negative, 5 = most positive). Write a function read_AFINN_dictionary to read in this file and create two dictionaries like that above -- one for positive terms and one for negative terms. How did you decide the cutoff point for positive and negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_AFINN_dictionary(filename):\n",
    "    positive=[]\n",
    "    negative=[]\n",
    "    \n",
    "    # Your code here\n",
    "    return set(positive), set(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_AFINN_dictionary(filename):\n",
    "    positive=[]\n",
    "    negative=[]\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            word=cols[0]\n",
    "            value=int(cols[1])\n",
    "            if value <= -2:\n",
    "                negative.append(word)\n",
    "            elif value >= 2:\n",
    "                positive.append(word)\n",
    "    \n",
    "    return set(positive), set(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive, negative=read_AFINN_dictionary(\"AFINN-111.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts=dictionary_document_count(tweets, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     date  document frequency\n",
      "0     2019-01-19 14:09:37                   0\n",
      "1     2019-01-19 13:50:09                   0\n",
      "2     2019-01-19 12:51:30                   1\n",
      "3     2019-01-19 12:11:38                   1\n",
      "4     2019-01-19 11:29:21                   1\n",
      "5     2019-01-19 03:24:49                   1\n",
      "6     2019-01-19 03:22:44                   0\n",
      "7     2019-01-19 03:22:29                   0\n",
      "8     2019-01-19 03:02:50                   1\n",
      "9     2019-01-19 02:23:46                   1\n",
      "10    2019-01-19 02:18:18                   1\n",
      "11    2019-01-19 02:18:14                   0\n",
      "12    2019-01-19 02:14:44                   0\n",
      "13    2019-01-19 02:14:11                   0\n",
      "14    2019-01-19 01:52:47                   0\n",
      "15    2019-01-19 01:50:03                   1\n",
      "16    2019-01-19 01:46:08                   0\n",
      "17    2019-01-19 01:44:59                   1\n",
      "18    2019-01-19 01:42:51                   0\n",
      "19    2019-01-19 01:16:18                   1\n",
      "20    2019-01-19 01:14:30                   1\n",
      "21    2019-01-19 01:12:08                   0\n",
      "22    2019-01-19 01:06:38                   1\n",
      "23    2019-01-18 22:51:00                   1\n",
      "24    2019-01-18 16:58:04                   0\n",
      "25    2019-01-18 16:00:07                   0\n",
      "26    2019-01-18 15:59:35                   0\n",
      "27    2019-01-18 15:58:37                   1\n",
      "28    2019-01-18 15:02:41                   1\n",
      "29    2019-01-18 14:13:56                   0\n",
      "...                   ...                 ...\n",
      "36553 2009-06-24 22:09:19                   0\n",
      "36554 2009-06-23 13:40:38                   0\n",
      "36555 2009-06-21 14:47:41                   0\n",
      "36556 2009-06-18 13:26:53                   0\n",
      "36557 2009-06-15 23:13:05                   0\n",
      "36558 2009-06-14 14:25:36                   0\n",
      "36559 2009-06-08 20:15:29                   0\n",
      "36560 2009-06-05 18:21:37                   1\n",
      "36561 2009-06-03 18:19:49                   0\n",
      "36562 2009-05-28 18:03:34                   0\n",
      "36563 2009-05-27 14:18:52                   0\n",
      "36564 2009-05-26 14:42:01                   0\n",
      "36565 2009-05-23 16:11:19                   0\n",
      "36566 2009-05-22 16:28:34                   0\n",
      "36567 2009-05-22 02:59:39                   0\n",
      "36568 2009-05-20 22:29:47                   0\n",
      "36569 2009-05-20 13:25:39                   0\n",
      "36570 2009-05-19 17:43:39                   0\n",
      "36571 2009-05-18 14:26:00                   0\n",
      "36572 2009-05-17 15:00:03                   1\n",
      "36573 2009-05-16 22:22:45                   0\n",
      "36574 2009-05-15 14:13:13                   0\n",
      "36575 2009-05-14 16:30:40                   0\n",
      "36576 2009-05-13 17:38:28                   0\n",
      "36577 2009-05-12 19:21:55                   1\n",
      "36578 2009-05-12 14:07:28                   0\n",
      "36579 2009-05-08 20:40:15                   0\n",
      "36580 2009-05-08 13:38:08                   0\n",
      "36581 2009-05-05 01:00:10                   0\n",
      "36582 2009-05-04 18:54:25                   0\n",
      "\n",
      "[36583 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### EMPATH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath #pip install empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = Empath()\n",
    "sentence = \"she yelled at her son\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ = lexicon.analyze(sentence, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories for the sentence: she yelled at her son:\n",
      "wedding\n",
      "family\n",
      "death\n",
      "youth\n",
      "children\n"
     ]
    }
   ],
   "source": [
    "print('Categories for the sentence: ' + sentence + ':')\n",
    "for key, value in categ.items():\n",
    "    if value != 0:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['help', 'office', 'dance', 'money', 'wedding', 'domestic_work', 'sleep', 'medical_emergency', 'cold', 'hate', 'cheerfulness', 'aggression', 'occupation', 'envy', 'anticipation', 'family', 'vacation', 'crime', 'attractive', 'masculine', 'prison', 'health', 'pride', 'dispute', 'nervousness', 'government', 'weakness', 'horror', 'swearing_terms', 'leisure', 'suffering', 'royalty', 'wealthy', 'tourism', 'furniture', 'school', 'magic', 'beach', 'journalism', 'morning', 'banking', 'social_media', 'exercise', 'night', 'kill', 'blue_collar_job', 'art', 'ridicule', 'play', 'computer', 'college', 'optimism', 'stealing', 'real_estate', 'home', 'divine', 'sexual', 'fear', 'irritability', 'superhero', 'business', 'driving', 'pet', 'childish', 'cooking', 'exasperation', 'religion', 'hipster', 'internet', 'surprise', 'reading', 'worship', 'leader', 'independence', 'movement', 'body', 'noise', 'eating', 'medieval', 'zest', 'confusion', 'water', 'sports', 'death', 'healing', 'legend', 'heroic', 'celebration', 'restaurant', 'violence', 'programming', 'dominant_heirarchical', 'military', 'neglect', 'swimming', 'exotic', 'love', 'hiking', 'communication', 'hearing', 'order', 'sympathy', 'hygiene', 'weather', 'anonymity', 'trust', 'ancient', 'deception', 'fabric', 'air_travel', 'fight', 'dominant_personality', 'music', 'vehicle', 'politeness', 'toy', 'farming', 'meeting', 'war', 'speaking', 'listen', 'urban', 'shopping', 'disgust', 'fire', 'tool', 'phone', 'gain', 'sound', 'injury', 'sailing', 'rage', 'science', 'work', 'appearance', 'valuable', 'warmth', 'youth', 'sadness', 'fun', 'emotional', 'joy', 'affection', 'traveling', 'fashion', 'ugliness', 'lust', 'shame', 'torment', 'economics', 'anger', 'politics', 'ship', 'clothing', 'car', 'strength', 'technology', 'breaking', 'shape_and_size', 'power', 'white_collar_job', 'animal', 'party', 'terrorism', 'smell', 'disappointment', 'poor', 'plant', 'pain', 'beauty', 'timidity', 'philosophy', 'negotiate', 'negative_emotion', 'cleaning', 'messaging', 'competing', 'law', 'friends', 'payment', 'achievement', 'alcohol', 'liquid', 'feminine', 'weapon', 'children', 'monster', 'ocean', 'giving', 'contentment', 'writing', 'rural', 'positive_emotion', 'musical'])\n"
     ]
    }
   ],
   "source": [
    "#available categories in empath\n",
    "print(categ.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of categories\n",
    "len(categ.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<50} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "ACCESS_TOKEN = '1088502318876762112-31dLKSsBF3w7LOwdY0CIbGlPEtcpfn'\n",
    "ACCESS_SECRET = 'YOMH1yazqXerT7sOZrB3mskfDbAMj8fTRsSFzaJcGPe9v'\n",
    "CONSUMER_KEY = 'A8p6EzGDQ0oPULhiZL6cpm6kO'\n",
    "CONSUMER_SECRET = 'zhO22pzZDmQAzGmOkFa6KHlURtE64ymmcmsBppgsGQBLSqdouk'\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "# Create the api to connect to twitter with your creadentials\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True, compression=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUR Productivity FEYNMAN- strategies: \n",
      "i) Stop trying to know-it-all. \n",
      "\n",
      "ii) Don't worry about what others are thin… https://t.co/A1RdJGC9hG\n",
      "Our responsibility is to do what we can, learn what we can, improve the solutions, and pass them on. https://t.co/Jn8SVW0ysN\n",
      "Science is like sex: sometimes something useful comes out, but that is not the reason we are doing it.\n",
      "If you cannot explain something in simple terms, you don't understand it. https://t.co/3LHPA2RuKA\n",
      "\"Science! It's just magic without the lies.\"\n"
     ]
    }
   ],
   "source": [
    "user = api.get_user(\"ProfFeynman\")\n",
    "tweets = api.user_timeline(screen_name = 'ProfFeynman', count = 5, include_rts = True)\n",
    "for status in tweets:\n",
    "    print(status.text)\n",
    "    sentences.append(status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUR Productivity FEYNMAN- strategies: \n",
      "i) Stop trying to know-it-all. \n",
      "\n",
      "ii) Don't worry about what others are thin… https://t.co/A1RdJGC9hG-------------------------------------------------------------------------------------------------------------------------------------------- {'neg': 0.107, 'neu': 0.776, 'pos': 0.117, 'compound': 0.0531}\n",
      "Our responsibility is to do what we can, learn what we can, improve the solutions, and pass them on. https://t.co/Jn8SVW0ysN------------------------------------------------------------------------------------------------------------------------------------------------------------ {'neg': 0.0, 'neu': 0.796, 'pos': 0.204, 'compound': 0.5574}\n",
      "Science is like sex: sometimes something useful comes out, but that is not the reason we are doing it.---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- {'neg': 0.0, 'neu': 0.821, 'pos': 0.179, 'compound': 0.4019}\n",
      "If you cannot explain something in simple terms, you don't understand it. https://t.co/3LHPA2RuKA--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "\"Science! It's just magic without the lies.\"-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for sentence in sentences:\n",
    "    vs = analyzer.polarity_scores(sentence)\n",
    "    print(\"{:-<280} {}\".format(sentence, str(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
